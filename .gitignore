
train <- read.table("train.csv", sep=",", header=TRUE)
test <- read.table("test.csv", sep=",", header=TRUE)

cat.var.names <- c(paste("Product_Info_", c(1:3,5:7), sep=""), paste("Employment_Info_", c(2,3,5), sep=""), paste("InsuredInfo_", 1:7, sep=""), paste("Insurance_History_", c(1:4,7:9), sep=""), "Family_Hist_1", paste("Medical_History_", c(2:14, 16:23, 25:31, 33:41), sep=""))
cont.var.names <- c("Product_Info_4", "Ins_Age", "Ht", "Wt", "BMI", "Employment_Info_1", "Employment_Info_4", "Employment_Info_6", "Insurance_History_5", "Family_Hist_2", "Family_Hist_3", "Family_Hist_4", "Family_Hist_5")
disc.var.names <- c("Medical_History_1", "Medical_History_15", "Medical_History_24", "Medical_History_32", paste("Medical_Keyword_", 1:48, sep=""))
train.cat <- train[, cat.var.names]
head(train.cat)

train.cat <- train[, cat.var.names]
test.cat <- test[, cat.var.names]

train.cont <- train[, cont.var.names]
test.cont <- test[, cont.var.names]

train.disc <- train[, disc.var.names]
test.disc <- test[, disc.var.names]

###2) borro Nas y separo en train,valid, test

library("e1071")
source("deleteNA.R")

#Incluyo en train.cont las etiquetas para poder hacer el modelo svm
train.cont$Response<-train$Response

#Elimino los NA
train.cont.NAout<-deleteNA(train.cont)

#Transformo las etiquetas de Response en 0,1 para
train.cont.NAout[(which(train.cont.NAout$Response<=4)),"Response"]<-0
train.cont.NAout[(which(train.cont.NAout$Response>4)),"Response"]<-1

#Sólo quiero tomar 7000 observaciones:
#train.cont.NAout[1:7000,]

#Creo train-valid-test:
index<-1:7000

ntrain_valid<-trunc(0.80*7000)
[1] 5600
ntest<-7000 - ntrain_valid
[1] 1400
ntrain<-trunc(0.75*ntrain_valid)
[1] 4200
nvalid<-ntrain_valid-ntrain
[1] 1400

#Tomo muestras aleatorias de estos conjuntos:
index<-1:7000
sample_all<-sample(index, 7000) #(muestra lineal, longitud)
head(sample_all)
[1] 5000  687 6199  124 3334 3558

train_index<-sample_all[1:ntrain]
valid_index<-sample_all[4201:5600]# valid_index<-sample_all[(ntrain+1):(ntrain+nvalid)]
test_index<-sample_all[5601:7000]# test_index<-sample_all[5601:7000]


train_set<-train.cont.NAout[train_index, ]
valid_set<-train.cont.NAout[valid_index, ]
test_set<-train.cont.NAout[test_index, ]


############################ METODO1 fórmula svm con round (SVM-Type: C-classification): ############################

svm.model<-svm(Response ~ .,data = train_set, type="C", kernel="linear")

svm.model

Call:
  svm(formula = Response ~ ., data = train_set, type = "C", kernel = "linear")


Parameters:
  SVM-Type:  C-classification 
SVM-Kernel:  linear 
cost:  1 
gamma:  0.07692308 

Number of Support Vectors:  2409

svm.pred<-(predict(svm.model, valid_set[,-14])) # ya no round y aumenta eficacia de 0.7035714 a 0.7321429

# for classification:
#class.weights = c(A = 0.5, B = 0.5)

z<-table(valid_set[,14],svm.pred)

#Metodo a) accuracy
sum(svm.pred==valid_set[,14])/length(valid_set[,14])
[1] 0.7321429

#Método b) Model accuracy rates

classAgreement(z)
$diag
[1] 0.7321429

$kappa
[1] 0

$rand
[1] 0.6075003

$crand
[1] 0



###################### METODO2 datos svm (SVM-Type:  eps-regression):######################

svm.model1<-svm(train_set[,-14],train_set[,14], cost =100, gamma =1)
svm.pred1<-round(predict(svm.model1, valid_set[,-14]))

z<-table(valid_set[,14],svm.pred1)
sum(svm.pred1==valid_set[,14])/length(valid_set[,14])
[1] 0.73

##Accuracy con ROC Metodo1####CONFUSIÓN ENTRE PREDICT(SVM) Y PREDICTION(SET, LABELSET)
install.packages("ROCR")
library(ROCR)

pred1 <- prediction(svm.pred1,valid_set[,14]) # para modelos de regresión
perf1 <- performance(pred1,"tpr","fpr")
plot(perf1)


#Sensibilidad vs especificidad:
perf2 <- performance(pred1, "sens", "spec")
plot(perf2)

#Precision/recall:
perf3 <- performance(pred1, "prec", "rec")
plot(perf3)

#área under the curve. No funciona,¿fpr.stop=0.5?                      ERROR
perf4 <- performance(pred1, "auc", "fpr",fpr.stop=0.5)
plot(perf4)

#varImp del paquete "caret": Se supone que implementa el oden de importancia
#de las variables en función del área bajo la curva la curva ROC arrojado por el modelo
#No funciona: ni para data frames ni para objetos svm
varImp(train_set, scale = FALSE)
Error in UseMethod("varImp") : 
  no applicable method for 'varImp' applied to an object of class "data.frame"



#filterVarImp #área sobre la curva ROC sobre cada clase
#ROC curve variable importance
RocImp<-filterVarImp(train_set[,-14],train_set[,14])
Overall
Product_Info_4      10.8213019
Ins_Age              9.9022647
Ht                   4.0570430
Wt                  14.4962180
BMI                 15.1838316
Employment_Info_1    1.1059848
Employment_Info_4    2.3022282
Employment_Info_6    0.7289609
Insurance_History_5  4.8563510
Family_Hist_2        0.7952438
Family_Hist_3        1.5826676
Family_Hist_4        2.4426247
Family_Hist_5        1.0844985

sum( 10.8213019, 9.9022647, 4.0570430, 14.4962180, 15.1838316, 1.1059848, 2.3022282, 0.7289609, 4.8563510, 0.7952438, 1.5826676, 2.4426247, 1.0844985)
[1] 69.35922
#No coincide con 0.73, seran distintas unidades¿Poqué no coinciden área y unidades?



#Tasa de errores de clasificación y combinación de parámetros para obtener los óptimos:
tobj <- tune.svm(Response ~ ., data = train_set, gamma = 10^(-6:-1), cost = 10^(1:2))
bestGamma <- tobj$best.parameters[[1]]
bestC <- tobj$best.parameters[[2]]

bestGamma
[1] 0.001

bestC
[1] 100

#Puedo plotear el error y como se ajusta entre xlab=gamma y ylab= "C"(ES CIEGO Y NO OTRCE GARANTÍAS)
plot(tobj, transform.x = log10, xlab = expression(log[10](gamma)), ylab = "C")

#Entreno la máqina con los parámetro óptimos que he obtenido: 
model_optim <- svm(Response ~ ., data = train_set, cost = bestC, gamma = bestGamma, cross = 10)
pred <- round(predict(model_optim, valid_set[,-14]))

#otra medida de accuracy classAgreement:
acc <- table(pred, valid_set[,14]) # hace la matriz de confusión al revés
# ojo, porque hace una matriz de confusión que contiene índices -2,-1,0,1,2 
pred    0    1
-2    0    1
-1    0    1
0    19    2
1   331 1046

classAgreement(acc)
$diag
[1] 0.0007142857

$kappa
[1] 0

$rand
[1] 0.6357173

$crand
[1] 0.04973076


############################ RADIAL KERNEL ############################

svm.model<-svm(Response ~ .,data = train_set, type="C")
> svm.model

Call:
  svm(formula = Response ~ ., data = train_set, type = "C")


Parameters:
  SVM-Type:  C-classification 
SVM-Kernel:  radial 
cost:  1 
gamma:  0.07692308 

Number of Support Vectors:  2452


svm.pred<-predict(svm.model, valid_set[,-14])
z<-table(valid_set[,14],svm.pred)
sum(svm.pred==valid_set[,14])/length(valid_set[,14])
[1] 0.7514286

classAgreement(z)
$diag
[1] 0.7514286

$kappa
[1] 0.1209743

$rand
[1] 0.6261656

$crand
[1] 0.07896822

##### PLOTEAR EN SVM #################
plot(svm.model,valid_set, Product_Info_4~Ins_Age)#VISUALIZAR REALCIÓN ENTRE DOS VAR PARA EL MODELO

####### OPTIMUM  MODEL ######################

tobj <- tune.svm(Response ~ ., data = train_set, gamma = 10^(-6:-1), cost = 10^(1:2))
> 
  > tobj

Parameter tuning of 'svm':
  
  - sampling method: 10-fold cross validation 

- best parameters:
  gamma cost
0.01  100

####################### svm con parámetro cost óptimo:#######################
svm.model<-svm(Response ~ .,data = train_set, type="C",cost = 100)
> svm.model

Call:
  svm(formula = Response ~ ., data = train_set, type = "C", cost = 100)


Parameters:
  SVM-Type:  C-classification 
SVM-Kernel:  radial 
cost:  100 
gamma:  0.07692308 

svm.pred<-predict(svm.model, valid_set[,-14])
> z<-table(valid_set[,14],svm.pred)
> sum(svm.pred==valid_set[,14])/length(valid_set[,14])
[1] 0.72

###################### con parámetros cost y gamma óptimos, obtengo así la mejor accurancy:######################

svm.model<-svm(Response ~ .,data = train_set, type="C",cost = 100, gamma = 0.01)
> svm.model

Call:
  svm(formula = Response ~ ., data = train_set, type = "C", cost = 100, gamma = 0.01)


Parameters:
  SVM-Type:  C-classification 
SVM-Kernel:  radial 
cost:  100 
gamma:  0.01 

Number of Support Vectors:  2254

> svm.pred<-predict(svm.model, valid_set[,-14])
> z<-table(valid_set[,14],svm.pred)
> sum(svm.pred==valid_set[,14])/length(valid_set[,14])
[1] 0.76

######Finalmente hago la predicción sobre el test_set 
######así compruebo si efectivamente la predicción era buena o fue 
#######acertada de un modo aletorio:
svm.model<-svm(Response ~ .,data = train_set, type="C",cost = 100, gamma = 0.01,cross = 10)
> svm.model

Call:
  svm(formula = Response ~ ., data = train_set, type = "C", cost = 100, 
      gamma = 0.01, cross = 10)


Parameters:
  SVM-Type:  C-classification 
SVM-Kernel:  radial 
cost:  100 
gamma:  0.01 

Number of Support Vectors:  2254

> svm.pred<-predict(svm.model, test_set[,-14])
> z<-table(test_set[,14],svm.pred)
> sum(svm.pred==test_set[,14])/length(test_set[,14])
[1] 0.7507143


############################################ POLINOMIAL KERNEL #######################################################

svm.model<-svm(Response ~ .,data = train_set, type="C",kernel="polynomial")
> 
  > svm.model

Call:
  svm(formula = Response ~ ., data = train_set, type = "C", kernel = "polynomial")


Parameters:
  SVM-Type:  C-classification 
SVM-Kernel:  polynomial 
cost:  1 
degree:  3 
gamma:  0.07692308 
coef.0:  0 

Number of Support Vectors:  2187

svm.pred<-predict(svm.model, valid_set[,-14])
> z<-table(valid_set[,14],svm.pred)
> sum(svm.pred==valid_set[,14])/length(valid_set[,14])
[1] 0.7628571

#####trato de elegir con tune los mejores parámetros, pero tarda muchoooo
tobj <- tune.svm(Response ~ ., data = train_set, kernel = "polynomial", gamma = 10^(-6:1), cost = 10^(-1:2))
           #Esta manera de hacerlo resulta ser demasiado costosa en tiempo
           #Puebo otrabest.tune(svm, Response ~ ., data = train_set, kernel = "polynomial")

#Sólo me ofrece un modelo SVM-Type:  eps-regression (para la clasificación peta) 
best.tune(svm, Response ~ ., data = train_set, kernel = "polynomial")

Call:
  best.tune(svm, Response ~ ., data = train_set, kernel = "polynomial")


Parameters:
  SVM-Type:  eps-regression 
SVM-Kernel:  polynomial 
cost:  1 
degree:  3 
gamma:  0.07692308 
coef.0:  0 
epsilon:  0.1 


Number of Support Vectors:  2459


######################################## SIGMOID KERNEL ###################################################
svm.model<-svm(Response ~ .,data = train_set, type="C",kernel="sigmoid")
svm.model

Call:
  svm(formula = Response ~ ., data = train_set, type = "C", kernel = "sigmoid")


Parameters:
  SVM-Type:  C-classification 
SVM-Kernel:  sigmoid 
cost:  1 
gamma:  0.07692308 
coef.0:  0 

Number of Support Vectors:  1670

svm.pred<-predict(svm.model, valid_set[,-14])
z<-table(valid_set[,14],svm.pred)
sum(svm.pred==valid_set[,14])/length(valid_set[,14])
[1] 0.6385714


#elijo mejores parámetros con tune para Kernel sigmoid:
tobj <- tune.svm(Response ~ ., data = train_set, kernel = "sigmoid", gamma = 10^(-6:-1), cost = 10^(1:2))
> tobj

Parameter tuning of 'svm':
  
  - sampling method: 10-fold cross validation 

- best parameters:
  gamma cost
1e-04   10

- best performance: 0.2293628 


# una vez seleccionados los mejores parámetros, hago el modelo con ellos
svm.model<-svm(Response ~ ., data = train_set,type = "C", kernel = "sigmoid",cost = 10, gamma = 0.0001, cross = 10)
> svm.model

Call:
  svm(formula = Response ~ ., data = train_set, type = "C", kernel = "sigmoid", cost = 10, gamma = 1e-04, 
      cross = 10)


Parameters:
  SVM-Type:  C-classification 
SVM-Kernel:  sigmoid 
cost:  10 
gamma:  1e-04 
coef.0:  0 

Number of Support Vectors:  2270

svm.pred<-predict(svm.model, valid_set[,-14])
> z<-table(valid_set[,14],svm.pred)
> sum(svm.pred==valid_set[,14])/length(valid_set[,14])
[1] 0.745

######Finalmente hago la predicción sobre el test_set 
######así compruebo si efectivamente la predicción era buena o fue 
#######acertada de un modo aletorio:
svm.model<-svm(Response ~ ., data = train_set,type = "C", kernel = "sigmoid",cost = 10, gamma = 0.0001, cross = 10)
> svm.model

Call:
  svm(formula = Response ~ ., data = train_set, type = "C", kernel = "sigmoid", cost = 10, gamma = 1e-04, 
      cross = 10)


Parameters:
  SVM-Type:  C-classification 
SVM-Kernel:  sigmoid 
cost:  10 
gamma:  1e-04 
coef.0:  0 

Number of Support Vectors:  2270

svm.pred<-predict(svm.model, test_set[,-14])
z<-table(test_set[,14],svm.pred)
sum(svm.pred==test_set[,14])/length(test_set[,14])
0.7485714
